import streamlit as st
import gspread
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO
from wordcloud import WordCloud
import itertools
from collections import Counter
from config import Config

st.set_page_config(page_title="LinkedIn Job Scraper", layout="centered")
st.title("LinkedIn Job Scraper (Streamlit)")
# streamlit run c:\Users\potre\OneDrive\LinkedIn_Automation\streamlit_linkedin_scraper.py [ARGUMENTS]
# --- Configuration from environment variables ---

def read_google_sheet(sheet_url=None, credentials=None):
    """Read Google Sheet with improved error handling."""
    # Use environment variables or fall back to secrets for compatibility
    if not sheet_url:
        sheet_url = Config.SHEET_URL or st.secrets.get("sheet_url", None)
    if not credentials:
        credentials = Config.CREDS_PATH or st.secrets.get("sheets_creds_path", None)
    
    if not sheet_url or not credentials:
        st.error("üìã Configuration missing")
        st.info("Please set LINKEDIN_SHEET_URL and LINKEDIN_CREDS_PATH in .env file")
        return pd.DataFrame()  # Return empty dataframe instead of crashing
    
    try:
        gc = gspread.service_account(filename=credentials)
        sh = gc.open_by_url(sheet_url)
        worksheet = sh.sheet1
        data = worksheet.get_all_records()
        return pd.DataFrame(data)
    except FileNotFoundError:
        st.error("üìÅ Credentials file not found")
        return pd.DataFrame()
    except Exception as e:
        st.error(f"‚ùå Failed to load data: {str(e)[:100]}")
        return pd.DataFrame()

# --- UI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (—Å–ø–æ–π–ª–µ—Ä) ---
with st.expander("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏)", expanded=False):
    config = Config.get_all()
    st.markdown("""
    **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
    - **Google Sheets URL:** `{}`
    - **Google Sheets Credentials:** `{}`
    - **Output File:** `{}`
    - **Telegram Configured:** `{}`
    - **Chrome Configured:** `{}`
    
    Configuration is loaded from environment variables.
    Create .env file based on .env.example to customize.
    """.format(
        config.get('sheet_url') or "[Not configured]",
        config.get('creds_path') or "[Not configured]",
        config.get('output_file_path') or "[Not configured]",
        "Yes" if config.get('telegram_bot_token') else "No",
        "Yes" if config.get('chromedriver_path') else "No"
    ))

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ ---
st.markdown("---")
st.header("–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ Google Sheets")
try:
    df = read_google_sheet()
    # === –§–ò–ö–°: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ ===
    for col in ["Elapsed Time (s)", "Job ID"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    if "Timestamp" in df.columns:
        df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors='coerce')
    bool_cols = [
        "Visa Sponsorship or Relocation",
        "Anaplan",
        "SAP APO",
        "Planning",
        "No Relocation Support",
        "Remote",
        "Remote Prohibited",
        "Already Applied"
    ]
    for col in bool_cols:
        if col in df.columns:
            df[col] = df[col].astype(str).str.upper().map({"TRUE": True, "FALSE": False})
    # === END FIX ===

    # --- –§–∏–ª—å—Ç—Ä—ã –¥–ª—è –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å –∫–Ω–æ–ø–∫–∞–º–∏ Select/Deselect All ---
    st.sidebar.header("–§–∏–ª—å—Ç—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏")
    all_companies = sorted(str(c).strip() for c in df['Company'].dropna().unique())
    all_skills = sorted(set(
        s.strip() for skills in df['Skills'].dropna()
        for s in str(skills).split(",") if s.strip()
    ))

    # –§–∏–ª—å—Ç—Ä –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º (selectbox)
    company_filter = st.sidebar.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º", options=["–í—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏"] + all_companies, index=0, key='company_filter_select')
    if company_filter == "–í—Å–µ –∫–æ–º–ø–∞–Ω–∏–∏":
        selected_companies = all_companies
    else:
        selected_companies = [company_filter]

    # –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–≤—ã–∫–∞–º (selectbox)
    skill_filter = st.sidebar.selectbox("–§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–≤—ã–∫–∞–º (Skills)", options=["–í—Å–µ –Ω–∞–≤—ã–∫–∏"] + all_skills, index=0, key='skill_filter_select')
    if skill_filter == "–í—Å–µ –Ω–∞–≤—ã–∫–∏":
        selected_skills = all_skills
    else:
        selected_skills = [skill_filter]

    # --- –ì–∞–ª–æ—á–∫–∞: —É–¥–∞–ª—è—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –≤–∞–∫–∞–Ω—Å–∏–π ---
    remove_duplicates = st.sidebar.checkbox("Remove vacancies duplicates (by Company + Vacancy Title)", value=True)
    dedup_priority = None
    if remove_duplicates:
        dedup_priority = st.sidebar.selectbox(
            "If duplicates: which to keep?",
            ["First event (any stage)", "Prefer TG message sent if exists"],
            index=1
        )

    filtered_df = df[
        df['Company'].isin(selected_companies)
    ]
    if selected_skills:
        filtered_df = filtered_df[filtered_df['Skills'].apply(lambda x: any(skill in str(x) for skill in selected_skills))]

    if remove_duplicates and "Company" in filtered_df.columns and "Vacancy Title" in filtered_df.columns:
        # –ü—Ä–∏–≤–µ—Å—Ç–∏ Company –∏ Vacancy Title –∫ —Å—Ç—Ä–æ–∫–µ –∏ —É–±—Ä–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        filtered_df["Company"] = filtered_df["Company"].astype(str).str.strip()
        filtered_df["Vacancy Title"] = filtered_df["Vacancy Title"].astype(str).str.strip()
        if dedup_priority == "First event (any stage)":
            filtered_df = filtered_df.drop_duplicates(subset=["Company", "Vacancy Title"], keep="first")
        elif dedup_priority == "Prefer TG message sent if exists":
            if "TG message sent" in filtered_df.columns:
                filtered_df["TG message sent"] = filtered_df["TG message sent"].astype(str).str.strip().str.lower()
                df_yes = filtered_df[filtered_df["TG message sent"] == "yes"].drop_duplicates(subset=["Company", "Vacancy Title"], keep="first")
                df_no = filtered_df[filtered_df["TG message sent"] != "yes"].drop_duplicates(subset=["Company", "Vacancy Title"], keep="first")
                pairs_yes = set(zip(df_yes["Company"], df_yes["Vacancy Title"]))
                df_no = df_no[~df_no.set_index(["Company", "Vacancy Title"]).index.isin(pairs_yes)]
                filtered_df = pd.concat([df_yes, df_no], ignore_index=True)
            else:
                filtered_df = filtered_df.drop_duplicates(subset=["Company", "Vacancy Title"], keep="first")

    # --- LIVE –ü–†–û–ì–†–ï–°–° ---
    st.sidebar.markdown("---")
    st.sidebar.header("Live-–ø—Ä–æ–≥—Ä–µ—Å—Å –ø–æ–∏—Å–∫–∞")
    total_jobs = len(filtered_df)
    filtered_jobs = len(filtered_df)
    st.sidebar.metric("–í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π", total_jobs)
    st.sidebar.metric("–í–∞–∫–∞–Ω—Å–∏–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤", filtered_jobs)
    st.sidebar.progress(filtered_jobs / total_jobs if total_jobs else 0)

    # --- FUNNEL: REAL PROGRESS FROM MAIN SHEET ---
    st.subheader("Search Funnel (All Stages)")
    funnel_counts = {}
    try:
        creds_path = Config.CREDS_PATH or st.secrets.get("sheets_creds_path", None)
        sheet_url = Config.SHEET_URL or st.secrets.get("sheet_url", None)
        if not creds_path or not sheet_url:
            raise ValueError("Configuration missing")
        gc = gspread.service_account(filename=creds_path)
        sh = gc.open_by_url(sheet_url)
        ws = sh.sheet1
        log_df = pd.DataFrame(ws.get_all_records())
        # === –§–ò–ö–°: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –≤—Å–µ—Ö –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ ===
        for col in ["Elapsed Time (s)", "Job ID"]:
            if col in log_df.columns:
                log_df[col] = pd.to_numeric(log_df[col], errors='coerce')
        if "Timestamp" in log_df.columns:
            log_df["Timestamp"] = pd.to_datetime(log_df["Timestamp"], errors='coerce')
        for col in bool_cols:
            if col in log_df.columns:
                log_df[col] = log_df[col].astype(str).str.upper().map({"TRUE": True, "FALSE": False})
        # === END FIX ===
        stages = [
            "Viewed",
            "Filtered (criteria)",
            "Passed filters",
            "TG message sent",
            "Filtered (already applied)"
        ]
        for stage in stages:
            if stage == "TG message sent":
                # –î–ª—è —ç—Ç–æ–≥–æ —ç—Ç–∞–ø–∞ —Å—á–∏—Ç–∞–µ–º –ø–æ –∫–æ–ª–æ–Ω–∫–µ TG message sent == 'yes', –µ—Å–ª–∏ –∫–æ–ª–æ–Ω–∫–∞ –µ—Å—Ç—å
                if "TG message sent" in log_df.columns:
                    funnel_counts[stage] = (log_df["TG message sent"].astype(str).str.lower() == "yes").sum()
                else:
                    funnel_counts[stage] = 0
            elif stage == "Passed filters":
                # Passed filters: —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ–≥–æ TG message sent != 'yes'
                if "TG message sent" in log_df.columns:
                    mask_passed = (log_df["Stage"] == stage)
                    mask_not_sent = (log_df["TG message sent"].astype(str).str.lower() != "yes")
                    funnel_counts[stage] = (mask_passed & mask_not_sent).sum()
                else:
                    funnel_counts[stage] = (log_df["Stage"] == stage).sum()
            else:
                funnel_counts[stage] = (log_df["Stage"] == stage).sum()
        percent_stages = ["Viewed", "Filtered (criteria)", "Passed filters", "TG message sent"]
        values = [funnel_counts[stage] for stage in percent_stages]
        labels = []
        prev = None
        for i, stage in enumerate(percent_stages):
            val = values[i]
            if prev is not None and prev > 0:
                percent = val / prev * 100
                labels.append(f"{stage} ({val}, {percent:.1f}%)")
            else:
                labels.append(f"{stage} ({val})")
            prev = val
        if funnel_counts["Filtered (already applied)"] > 0:
            labels.append(f"Filtered (already applied) ({funnel_counts['Filtered (already applied)']})")
            values.append(funnel_counts["Filtered (already applied)"])
        fig, ax = plt.subplots(figsize=(8, 5))
        bars = ax.barh(range(len(values)), values, color=plt.cm.viridis([i / len(values) for i in range(len(values))]))
        ax.set_yticks(range(len(labels)))
        ax.set_yticklabels(labels)
        ax.invert_yaxis()
        ax.set_xlabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
        ax.set_title("–í–æ—Ä–æ–Ω–∫–∞ –ø–æ —ç—Ç–∞–ø–∞–º")
        for i, bar in enumerate(bars):
            ax.text(bar.get_width() + max(values) * 0.01, bar.get_y() + bar.get_height()/2, str(values[i]), va='center')
        st.pyplot(fig)

        # --- –î–µ—Ç–∞–ª—å–Ω–∞—è –º–µ—Ö–∞–Ω–∏–∫–∞ —ç—Ç–∞–ø–æ–≤ –≤–æ—Ä–æ–Ω–∫–∏ –ø–æ–¥ —Å–ø–æ–π–ª–µ—Ä–æ–º ---
        with st.expander("–ö–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø –≤–æ—Ä–æ–Ω–∫–∏? (–∫–ª–∏–∫–Ω–∏—Ç–µ –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π)"):
            st.markdown("""
            **–ú–µ—Ö–∞–Ω–∏–∫–∞ —Ä–∞—Å—á—ë—Ç–∞ —ç—Ç–∞–ø–æ–≤ –≤–æ—Ä–æ–Ω–∫–∏:**

            - **Viewed** ‚Äî –í–∞–∫–∞–Ω—Å–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–∞ –ø–∞—Ä—Å–µ—Ä–æ–º –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ª–æ–≥. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ —Ñ–∏–∫—Å–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏ —Ñ–ª–∞–≥–∏ (—Ä–µ–ª–æ–∫–∞—Ü–∏—è, —É–¥–∞–ª—ë–Ω–∫–∞, Remote Prohibited –∏ –¥—Ä.).
            - **Filtered (criteria)** ‚Äî –í–∞–∫–∞–Ω—Å–∏—è –ø—Ä–æ—à–ª–∞ –ø–µ—Ä–≤–∏—á–Ω—É—é —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
                - –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (Visa/Relocation, Anaplan, SAP APO, Planning –∏ –¥—Ä.)
                - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–µ–ª–æ–∫–∞—Ü–∏–∏/–≤–∏–∑—ã
                - –£–¥–∞–ª—ë–Ω–∫–∞ (Remote/Remote Prohibited)
                - –ù–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ä–µ–ª–æ–∫–∞—Ü–∏–∏ (No Relocation Support)
            - **Filtered (already applied)** ‚Äî –í–∞–∫–∞–Ω—Å–∏—è –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞, —Ç.–∫. —Ä–∞–Ω–µ–µ —É–∂–µ –±—ã–ª–∞ –ø–æ–¥–∞–Ω–∞ –∑–∞—è–≤–∫–∞ (Already Applied = True).
            - **Passed filters** ‚Äî –í–∞–∫–∞–Ω—Å–∏—è –ø—Ä–æ—à–ª–∞ –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏ –ø—Ä–∏–∑–Ω–∞–Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π (–ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º). –¢–∞–∫–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ–ø–∞–¥–∞—é—Ç –≤ –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π.
            - **TG message sent** ‚Äî –î–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –≤ Telegram.

            **–ü–æ—á–µ–º—É –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ —ç—Ç–∞–ø–∞—Ö?**
            - –ù–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –º–æ–≥—É—Ç –æ—Ç—Å–µ–∫–∞—Ç—å—Å—è –ø–æ —Ä–∞–∑–Ω—ã–º –ø—Ä–∏—á–∏–Ω–∞–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ —Å–æ–≤–ø–∞–ª–∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, –Ω–µ—Ç —Ä–µ–ª–æ–∫–∞—Ü–∏–∏, —É–∂–µ –ø–æ–¥–∞–≤–∞–ª–∏—Å—å –∏ —Ç.–¥.).
            - –í—Å–µ —ç—Ç–∞–ø—ã –ª–æ–≥–∏—Ä—É—é—Ç—Å—è –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏: –æ–¥–Ω–∞ –∏ —Ç–∞ –∂–µ –≤–∞–∫–∞–Ω—Å–∏—è –º–æ–∂–µ—Ç –ø–æ—è–≤–∏—Ç—å—Å—è –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–∞—Ö, –Ω–æ –≤ –∞–Ω–∞–ª–∏—Ç–∏–∫–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –ø–æ —Ç–µ–∫—É—â–µ–º—É —ç—Ç–∞–ø—É.
            - –ü–æ–¥—Ä–æ–±–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∞ –≤ README.md (—Ä–∞–∑–¥–µ–ª "–õ–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ –º—ç—Ç—á–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–π").
            """)
    except Exception as e:
        st.info(f"Unable to load funnel from main sheet: {e}")

    # --- BAR CHART: –°–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ–∏—Å–∫–∞ ---
    st.subheader("–°–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ–∏—Å–∫–∞")
    criteria_columns = [
        "Visa Sponsorship or Relocation",
        "Anaplan",
        "SAP APO",
        "Planning",
        "No Relocation Support",
        "Remote",
        "Remote Prohibited",
        "Already Applied"
    ]
    present_criteria = [col for col in criteria_columns if col in log_df.columns]
    if present_criteria:
        matches = {col: (log_df[col].astype(str).str.upper() == "TRUE").sum() for col in present_criteria}
        fig_crit, ax_crit = plt.subplots(figsize=(8, 4))
        ax_crit.bar(matches.keys(), matches.values(), color=plt.cm.Paired.colors)
        ax_crit.set_ylabel("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        ax_crit.set_xlabel("–ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–∏—Å–∫–∞")
        ax_crit.set_title("–í–∞–∫–∞–Ω—Å–∏–∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –ø–æ–∏—Å–∫–∞")
        ax_crit.set_xticklabels(matches.keys(), rotation=30, ha='right')
        for i, v in enumerate(matches.values()):
            ax_crit.text(i, v + max(matches.values()) * 0.01, str(v), ha='center', va='bottom')
        st.pyplot(fig_crit)
    else:
        st.info("–í Google Sheets –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ —Å –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –ø–æ–∏—Å–∫–∞ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞.")

    # --- TAG CLOUD: matched key words ---
    st.subheader("–û–±–ª–∞–∫–æ —Ç–µ–≥–æ–≤: —Å–æ–≤–ø–∞–≤—à–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (Matched key words)")
    matched_keywords_col = None
    for col in log_df.columns:
        if col.lower().replace('_', ' ').startswith('matched key'):
            matched_keywords_col = col
            break
    if matched_keywords_col:
        all_keywords = list(itertools.chain.from_iterable(
            [s.strip() for s in str(keywords).split(",") if s.strip()] for keywords in log_df[matched_keywords_col].dropna()
        ))
        keyword_counts = Counter(all_keywords)
        if keyword_counts:
            fig_kw, ax_kw = plt.subplots(figsize=(12, 5))
            wc_kw = WordCloud(width=900, height=400, background_color='white', colormap='plasma',
                              max_words=100, prefer_horizontal=1.0, collocations=False).generate_from_frequencies(keyword_counts)
            ax_kw.imshow(wc_kw, interpolation='bilinear')
            ax_kw.axis('off')
            st.pyplot(fig_kw)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±–ª–∞–∫–∞ —Å–æ–≤–ø–∞–≤—à–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤.")
    else:
        st.info("–í Google Sheets –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å —Å–æ–≤–ø–∞–≤—à–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –æ–±–ª–∞–∫–∞ —Ç–µ–≥–æ–≤.")

    # --- BAR CHART: Remote Prohibited ---
    st.subheader("–í–∞–∫–∞–Ω—Å–∏–∏ —Å –∑–∞–ø—Ä–µ—Ç–æ–º —É–¥–∞–ª—ë–Ω–∫–∏ (Remote Prohibited)")
    if 'Remote Prohibited' in log_df.columns:
        st.write(f"–í—Å–µ–≥–æ –≤–∞–∫–∞–Ω—Å–∏–π —Å –∑–∞–ø—Ä–µ—Ç–æ–º –Ω–∞ —É–¥–∞–ª—ë–Ω–∫—É: {(log_df['Remote Prohibited']==True).sum()}")
        st.bar_chart(log_df['Remote Prohibited'].value_counts())
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–µ—Ç—É —É–¥–∞–ª—ë–Ω–∫–∏.")

    # --- Filter main data for all analytics by Stage ---
    filtered_df = filtered_df[filtered_df.get("Stage", "Passed filters") == "Passed filters"] if "Stage" in filtered_df.columns else filtered_df

    # --- Heatmap –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ ---
    st.subheader("–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–¥–µ–Ω—å/15-–º–∏–Ω—É—Ç–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª)")
    if 'Timestamp' in filtered_df.columns:
        dt = pd.to_datetime(filtered_df['Timestamp'], errors='coerce')
        if not dt.isnull().all():
            filtered_df['weekday'] = dt.dt.day_name()
            # –î–æ–±–∞–≤–ª—è–µ–º 15-–º–∏–Ω—É—Ç–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
            filtered_df['quarter'] = dt.dt.hour.astype(str).str.zfill(2) + ':' + (dt.dt.minute // 15 * 15).astype(str).str.zfill(2)
            heatmap_time = pd.pivot_table(filtered_df, index='weekday', columns='quarter', values='Vacancy Title', aggfunc='count', fill_value=0)
            # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–Ω–µ–π –Ω–µ–¥–µ–ª–∏ –∏ –≤—Ä–µ–º–µ–Ω–∏
            weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
            heatmap_time = heatmap_time.reindex(weekday_order)
            fig_time, ax_time = plt.subplots(figsize=(16, 5))
            sns.heatmap(heatmap_time, cmap='YlOrRd', ax=ax_time)
            ax_time.set_title('Heatmap: –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –¥–Ω—è–º –∏ 15-–º–∏–Ω—É—Ç–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞–º')
            st.pyplot(fig_time)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è heatmap –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.")
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è heatmap –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.")

    # --- –¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π –∏ –≤–∞–∫–∞–Ω—Å–∏–π ---
    st.subheader("–¢–æ–ø –∫–æ–º–ø–∞–Ω–∏–π –∏ –≤–∞–∫–∞–Ω—Å–∏–π")
    if 'Vacancy Title' in filtered_df.columns:
        st.write("**–¢–æ–ø-10 –≤–∞–∫–∞–Ω—Å–∏–π:**")
        st.dataframe(filtered_df['Vacancy Title'].value_counts().head(10).reset_index().rename(columns={'index': '–í–∞–∫–∞–Ω—Å–∏—è', 'Vacancy Title': '–ß–∞—Å—Ç–æ—Ç–∞'}), hide_index=True)

    # --- Heatmap: —Ç–æ–ø –Ω–∞–≤—ã–∫–æ–≤ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º (skills) ---
    st.subheader("Heatmap: —Ç–æ–ø –Ω–∞–≤—ã–∫–æ–≤ –ø–æ –∫–æ–º–ø–∞–Ω–∏—è–º (skills)")
    if 'Skills' in filtered_df.columns and 'Company' in filtered_df.columns:
        all_skills = list(itertools.chain.from_iterable(
            [s.strip() for s in str(skills).split(",") if s.strip()] for skills in filtered_df['Skills'].dropna()
        ))
        most_common_skills = [w for w, _ in Counter(all_skills).most_common(10)]
        skill_matrix = pd.DataFrame(0, index=filtered_df['Company'].unique(), columns=most_common_skills)
        for _, row in filtered_df.iterrows():
            company = row['Company']
            for skill in [s.strip() for s in str(row['Skills']).split(",") if s.strip()]:
                if skill in skill_matrix.columns:
                    skill_matrix.at[company, skill] += 1
        if not skill_matrix.empty and skill_matrix.values.sum() > 0:
            fig3, ax3 = plt.subplots(figsize=(10, max(3, len(skill_matrix)//2)))
            sns.heatmap(skill_matrix, annot=True, fmt='d', cmap='YlGnBu', ax=ax3)
            st.pyplot(fig3)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è heatmap –ø–æ –Ω–∞–≤—ã–∫–∞–º.")
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è heatmap –ø–æ –Ω–∞–≤—ã–∫–∞–º.")

    # --- Tag Cloud –ø–æ –Ω–∞–≤—ã–∫–∞–º (Skills) ---
    st.subheader("–û–±–ª–∞–∫–æ –Ω–∞–≤—ã–∫–æ–≤ (Skills Tag Cloud)")
    if 'Skills' in filtered_df.columns:
        all_skills = list(itertools.chain.from_iterable(
            [s.strip() for s in str(skills).split(",") if s.strip()] for skills in filtered_df['Skills'].dropna()
        ))
        skill_counts = Counter(all_skills)
        if skill_counts:
            wc = WordCloud(width=900, height=400, background_color='white', colormap='viridis',
                           max_words=100, prefer_horizontal=1.0, collocations=False).generate_from_frequencies(skill_counts)
            fig_wc, ax_wc = plt.subplots(figsize=(12, 5))
            ax_wc.imshow(wc, interpolation='bilinear')
            ax_wc.axis('off')
            st.pyplot(fig_wc)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±–ª–∞–∫–∞ –Ω–∞–≤—ã–∫–æ–≤.")
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ–±–ª–∞–∫–∞ –Ω–∞–≤—ã–∫–æ–≤.")

    # --- User-friendly —Ç–∞–±–ª–∏—Ü–∞ ---
    if "Job URL" in filtered_df.columns:
        filtered_df["Job Link"] = filtered_df["Job URL"].apply(lambda x: f"[–û—Ç–∫—Ä—ã—Ç—å –≤–∞–∫–∞–Ω—Å–∏—é]({x})" if pd.notnull(x) and x else "")
        display_cols = [col for col in filtered_df.columns if col not in ["Job URL"]]
        if "Job Link" not in display_cols:
            display_cols.append("Job Link")
        st.dataframe(filtered_df[display_cols], use_container_width=True, height=420, hide_index=True)
    else:
        st.dataframe(filtered_df, use_container_width=True, height=420, hide_index=True)

    # --- –ì—Ä–∞—Ñ–∏–∫ –ø–æ –¥–Ω—è–º ---
    if 'Timestamp' in filtered_df.columns:
        filtered_df['Date'] = pd.to_datetime(filtered_df['Timestamp'], errors='coerce').dt.date
        daily_counts = filtered_df.groupby('Date').size()
        if not daily_counts.empty:
            st.line_chart(daily_counts)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ –¥–Ω—è–º.")
    else:
        st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –ø–æ –¥–Ω—è–º.")

    # Show helpful message if data is empty
    if df.empty:
        st.warning("üì≠ No data found. Check your Google Sheets connection or ensure the sheet has data.")
except Exception as e:
    st.error(f"‚ùå Error reading Google Sheets: {str(e)[:200]}")
    st.info("üí° Tip: Check your .env file configuration and internet connection")
