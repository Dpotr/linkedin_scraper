# LinkedIn Scraper V 2.2

**LinkedIn Scraper** — это мощный инструмент для автоматизированного поиска, анализа и сбора вакансий с LinkedIn с поддержкой семантического поиска, аналитики и отправки результатов в Telegram.

---

## Changelog

Полный список изменений см. в файле [CHANGELOG.md](CHANGELOG.md)

- **v2.3 (2025-04-21):**
    - Надёжное извлечение Job URL для всех карточек LinkedIn: поддержка любых вариантов структуры карточки, универсальный селектор, автоматическое преобразование относительных ссылок в абсолютные.
    - Теперь в Excel, Google Sheets, Telegram и Streamlit всегда отображается корректная ссылка на вакансию.
    - Улучшено логирование ошибок поиска ссылки: если структура карточки изменилась, HTML сохраняется в лог для быстрой диагностики.
    - См. подробности в CHANGELOG.md
- **v2.2 (2025-04-21):**
    - Улучшена логика парсинга: теперь для каждой вакансии логируются ВСЕ этапы (Viewed, Filtered, Passed filters, TG message sent) отдельными строками. Все ключевые флаги (включая Remote Prohibited) фиксируются для каждой вакансии на каждом этапе.
    - Новая логика: вакансии с флагом "Remote Prohibited" не отсекаются, а отмечаются для аналитики и информирования. Вся аналитика строится по этим флагам, но ни одна релевантная вакансия (например, с визой/релокацией) не пропускается.
    - Весь процесс выбора мэтчей по вакансиям стал полностью прозрачным: в Google Sheets и Excel для каждой вакансии и этапа видны все критерии и matched key words.
    - Улучшена документация, обновлён README (см. ниже).
    - См. подробности в CHANGELOG.md
- **v2.1 (2025-04-21):**
    - Все этапы вакансий (Viewed, Filtered, Passed filters, TG message sent) логируются отдельными строками в основной вкладке Google Sheets, все колонки всегда заполнены
    - Добавлена колонка 'TG message sent' для отслеживания отправки уведомлений в Telegram
    - В дашборде Streamlit добавлен чекбокс для удаления дубликатов вакансий (по Company + Vacancy Title)
    - Вся аналитика и воронка строятся по колонке Stage из основного листа
    - Оптимизация под лимиты Google Sheets API (всё в одном листе)
    - См. подробности в CHANGELOG.md
- **v2.0 (2025-04-21):**
    - Новый Streamlit-дэшборд с интерактивными фильтрами (компании, вакансии, навыки), кнопками Select/Deselect All и облаком тегов (skills tag cloud)
    - Улучшенная аналитика: все графики строятся по отфильтрованным данным
    - Проверка дубликатов при экспорте в Google Sheets по ключу "Vacancy Title - Company"
    - Heatmap и облако тегов теперь используют колонку Skills
    - Оптимизация скорости и стабильности, улучшенные сообщения об ошибках
    - См. подробности в CHANGELOG.md

---

## Возможности

- Поиск вакансий по ключевым словам (в том числе: удалёнка, Anaplan, SAP, планирование и др.)
- Семантический анализ описаний вакансий с помощью Sentence Transformers
- Сбор и анализ данных в Excel и Google Sheets
- Автоматическая отправка уведомлений и графиков в Telegram
- Графический интерфейс (Tkinter) для удобного запуска и настройки
- Поддержка различных сценариев фильтрации (релокация, удалёнка, опыт, навыки и пр.)
- Встроенные инструменты аналитики (графики распределения, p-chart, анализ навыков и "красных флагов", Streamlit dashboard)
- Гибкая настройка через параметры интерфейса

## Логика парсинга и мэтчинга вакансий

1. **Загрузка страницы с вакансиями** — скрипт скроллит страницу до полной загрузки всех карточек.
2. **Извлечение информации** — для каждой вакансии извлекается название, компания, описание, ссылка и т.д.
3. **Проверка по ключевым словам** — для каждой вакансии анализируется наличие ключевых слов по категориям:
    - Visa/Relocation (KEYWORDS_VISA)
    - Anaplan (KEYWORDS_ANAPLAN)
    - SAP APO (KEYWORDS_SAP)
    - Planning (KEYWORDS_PLANNING)
    - No Relocation Support (NO_RELOCATION_REQUIREMENTS)
    - Remote (REMOTE_REQUIREMENTS)
    - Remote Prohibited (REMOTE_PROHIBITED)
4. **Логирование этапа "Viewed"** — для каждой вакансии логируются найденные ключевые слова и значения всех критериев (флаги True/False) в Google Sheets/Excel.
5. **Фильтрация** — вакансии проходят фильтры:
    - Если уже подавались (Already Applied) — логируется этап "Filtered (already applied)"
    - Если не прошла по основным условиям (например, нет remote/relocation и нет ключевых навыков) — этап "Filtered (criteria)"
    - Если вакансия релевантна — этап "Passed filters"
6. **Remote Prohibited** — если в описании есть фразы из REMOTE_PROHIBITED, вакансия не отсекается, а только отмечается и попадает в аналитику.
7. **Matched key words** — для каждой вакансии сохраняется строка с совпавшими ключевыми словами.
8. **Отправка в Telegram** — для релевантных вакансий отправляется уведомление с аналитикой.
9. **Вся аналитика (Streamlit)** — строится по всем этапам и флагам, включая "Remote Prohibited".

## Структура проекта

- `universal parser_new_preprod(semantic_catgpt).py` — основной скрипт с поддержкой семантического поиска (рекомендуется для использования)
- `universal parser_wo_semantic_claude.py` — версия скрипта без семантического поиска, но с расширенными аналитическими функциями
- `universal parser_wo_semantic_chatgpt.py` — облегчённая версия без семантики
- `streamlit_linkedin_scraper.py` — интерактивный дашборд аналитики (Streamlit)
- `archive/` — архивные или вспомогательные файлы
- `companies_usa_remote.xlsx` — пример выходного файла с результатами
- `README.md` — описание проекта
- `CHANGELOG.md` — история изменений

## Установка

1. **Клонируйте репозиторий:**
   ```sh
   git clone https://github.com/Dpotr/linkedin_scraper.git
   cd linkedin_scraper
   ```

2. **Создайте виртуальное окружение и активируйте его:**
   ```sh
   python -m venv venv
   venv\Scripts\activate    # для Windows
   ```

3. **Установите зависимости:**
   ```sh
   pip install -r requirements.txt
   ```
   или вручную:
   ```sh
   pip install pandas requests matplotlib openpyxl selenium langdetect undetected-chromedriver sentence-transformers streamlit wordcloud
   ```

4. **Скачайте и установите ChromeDriver**  
   [Инструкция](https://chromedriver.chromium.org/downloads)  
   Укажите путь до chromedriver в интерфейсе или настройках скрипта.

## Быстрый старт

1. Запустите основной скрипт:
   ```sh
   python "universal parser_new_preprod(semantic_catgpt).py"
   ```
2. Для аналитики — запустите дашборд:
   ```sh
   streamlit run streamlit_linkedin_scraper.py
   ```
3. В графическом интерфейсе укажите параметры поиска, путь к профилю Chrome, токен Telegram-бота, ID чата и другие опции.
4. Нажмите "Start Scraper".

## Конфигурация и безопасность

- Все параметры (страна, ключевые слова, путь к Excel, токен Telegram, путь к ChromeDriver и профилю) настраиваются через GUI.
- Для семантического поиска требуется интернет для загрузки модели Sentence Transformers (кэшируется на диск).
- **tg_config.json** и **google_sheets_credentials.json** — содержат приватные ключи и токены. **Никогда не выкладывайте их в публичный репозиторий!** Добавьте их в .gitignore, если делаете проект публичным.

## Выходные данные

- Результаты сохраняются в Excel-файл.
- В Telegram отправляются текстовые уведомления и аналитические графики (bar chart, p-chart, skills chart).

## Требования

- Python 3.8+
- Google Chrome и соответствующий ChromeDriver
- Аккаунт Telegram для получения уведомлений

## Безопасность

- Не храните свои токены и личные данные в публичных репозиториях!
- Добавьте файл `.gitignore` для исключения секретов, кэша, виртуальных окружений и выходных данных.

## Пример .gitignore

```
venv/
__pycache__/
*.pyc
*.xlsx
*.log
*.env
*.db
.DS_Store
archive/
tg_config.json
google_sheets_credentials.json
```

## Лицензия

MIT License
